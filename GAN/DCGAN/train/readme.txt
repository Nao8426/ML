main.py: 学習を回すメインプログラム．学習を行う際はこのプログラムを実行．
train.py: 学習内容（パラメータや順序など）を設定．
network.py: ジェネレータとディスクリミネータの層構造．
load.py: データセットを読み込む際の設定．
util.py: 環境出力など，その他の自作関数．

########## 備考 ##########
"Batch Normalization"をディスクリミネータだけに置くか，ジェネレータにも置くかなど，"Batch Normalization"の位置には諸説あるが，とりあえずディスクリミネータだけに置くのがよいっぽいので，とりあえずそれで実装してます．
ジェネレータの出力層，ディスクリミネータの入力層には"Batch Normalization"を置かない方がよい（学習が不安定になる）らしい．
"Batch Normalization"よりも　"Spectral Normalization"の方が学習が安定するかも．（検証した感じ多分そう）
Spectral Normalizationを使用する場合は"nn.BatchNorm2d(channel)"を消して"nn.utils.spectral_norm(module)"を追加していけばよい．（"module"は層をそのまま囲めばOK）
あまり分かってないが，"detach"による勾配情報の削除は"G/D"の順で更新するのであれば不要・・・？（多分，"detach"以前のモデルに対して勾配情報が伝わらないようにするので，"D/G"の順で更新する場合は必要な時があるかも？）
テンプレートではディスクリミネータにおける畳み込み層を(K,S,P)=(3,2,1)としているので，画像の高さ（または幅）が奇数の場合はズレが生じる．(K,S,P)=(4,2,1)とすれば問題ないが，「カーネルサイズを奇数にすると中心のピクセルが生まれ，畳み込みの計算が有効に機能しやすい」という記述が散見されたので，こうしています．
